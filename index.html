<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation and Understanding">
  <meta property="og:title" content="SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation and Understanding"/>
  <meta property="og:description" content="SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation and Understanding"/>
  <meta property="og:url" content="https://Dekai21.github.io/SPIRAL/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="images/overview.jpg" /> -->
  <!-- <meta property="og:image:width" content="1245"/> -->
  <!-- <meta property="og:image:height" content="562"/> -->


  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="images/overview.jpg"> -->
  <!-- <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Point Cloud Generation, Diffusion Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation and Understanding</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.icon"> -->
  <link rel="icon" type="image/x-icon" href="images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation and Understanding</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=rehMAJUAAAAJ&view_op=list_works" target="_blank">Dekai Zhu<sup>1,4,*</sup></a>,</span> &nbsp;
              <span class="author-block">
              <a href="https://yixuanhu1.github.io/" target="_blank">Yixuan Hu<sup>1,*</sup></a>,</span> &nbsp;
              <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=J9a48hMAAAAJ&view_op=list_works" target="_blank">Youquan Liu<sup>2</sup></a>,</span> &nbsp;
              <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=bhEJsGoAAAAJ&view_op=list_works" target="_blank">Dongyue Lu<sup>3</sup></a>,</span> &nbsp;
              <span class="author-block">
              <a href="https://scholar.google.com/citations?user=-j1j7TkAAAAJ" target="_blank">Lingdong Kong<sup>3</sup></a>,</span> &nbsp;
              <span class="author-block">
              <a href="https://scholar.google.de/citations?hl=en&user=ELOVd8sAAAAJ&view_op=list_works" target="_blank">Slobodan Ilic<sup>1</sup></a></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup> Technical University of Munich, <sup>2</sup> Fudan University,<br>
              <sup>3</sup> National University of Singapore, <sup>4</sup> Munich Center for Machine Learning<br>
              <span class="eql-cntrb"><small>(* Equal Contribution)</small>
                <!-- <br> Under reviewed. </span> -->
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small> -->
                <p style="font-weight: bold;">NeurIPS 2025</p>
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                    <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://drive.google.com/file/d/1uw1Bpg8kHddpyLIcapvHfTbQhM1gDbec/view?usp=sharing" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code (coming soon)</span>
                </a>
              </span>

              <span class="link-block">
                <a href="" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-play-circle"></i>
                </span>
                <span>Video</span>
                </a>
              </span>

              <!-- <span class="link-block">
                <a href="https://www.bilibili.com/video/BV1b23dzUEF4/?share_source=copy_web&vd_source=d484dd59f507d51710a45ec873857c76" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-play-circle"></i>
                </span>
                <span>视频</span>
                </a>
              </span> -->

              <!-- ArXiv abstract Link -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2301.02561" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv (old version)</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero teaser" id="Demo">
  <div class="container is-max-desktop">
      <div class="hero-body has-text-centered">
      <video poster="" id="tree" autoplay controls loop height="100%">
        <source src="images/mvn_dk.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>
</section> -->


<!-- Paper abstract -->
<section class="section hero" id="Abstract">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified is-size-5">
          <p>
            Leveraging recent diffusion models, LiDAR-based large-scale 3D scene generation has achieved great success. 
            While recent voxel-based approaches can generate both geometric structures and semantic labels, existing range-view methods are limited to producing unlabeled LiDAR scenes. 
            Relying on pretrained segmentation models to predict the semantic maps often results in suboptimal cross-modal consistency. 
            To address this limitation while preserving the advantages of range-view representations, such as computational efficiency and simplified network design, 
            we propose <strong>SPIRAL</strong>, a <span style="color: #1e90ff">novel range-view LiDAR diffusion model</span> that simultaneously generates <span style="color: #1e90ff">depth, reflectance images, and semantic maps</span>. 
            Furthermore, we introduce <span style="color: #1e90ff">novel semantic-aware metrics</span> to evaluate the quality of the generated labeled range-view data. 
            Experiments on the SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves state-of-the-art performance with the smallest parameter size, 
            outperforming two-step methods that combine the generative and segmentation models. 
            Additionally, we validate that range images generated by <strong>SPIRAL</strong> can be effectively used for synthetic data augmentation in the downstream segmentation training, 
            significantly reducing the labeling effort on LiDAR data.
          </p>
        </div>
      </div>
    </div>
  </div>
  <br />
  <div class="container is-max-desktop">
    <!-- <div class="columns is-centered is-four-fifths">
      <div class="column is-four-fifths is-centered"> -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-five-fifths">
          <img src="images/teaser.png" alt="MY ALT TEXT" style="width: 100%;"/>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified is-size-5">
          <p>
            <strong>Figure 1:</strong> Visualization of LiDAR scenes and their semantic labels jointly generated by <strong>SPIRAL</strong>,
            exhibiting high geometric fidelity and semantic-geometric consistency.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero" id="Method">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <!-- <p>
            Leveraging recent diffusion models, LiDAR-based large-scale 3D scene generation has achieved great success. 
            While recent voxel-based approaches can generate both geometric structures and semantic labels, existing range-view methods are limited to producing unlabeled LiDAR scenes. 
            Relying on pretrained segmentation models to predict the semantic maps often results in suboptimal cross-modal consistency. 
            To address this limitation while preserving the advantages of range-view representations, such as computational efficiency and simplified network design, 
            we propose <strong>SPIRAL</strong>, a <span style="color: #1e90ff">novel range-view LiDAR diffusion model</span> that simultaneously generates <span style="color: #1e90ff">depth, reflectance images, and semantic maps</span>. 
            Furthermore, we introduce <span style="color: #1e90ff">novel semantic-aware metrics</span> to evaluate the quality of the generated labeled range-view data. 
            Experiments on the SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves state-of-the-art performance with the smallest parameter size, 
            outperforming two-step methods that combine the generative and segmentation models. 
            Additionally, we validate that range images generated by <strong>SPIRAL</strong> can be effectively used for synthetic data augmentation in the downstream segmentation training, 
            significantly reducing the labeling effort on LiDAR data.
          </p> -->
        </div>
      </div>
    </div>
  </div>
  <br />
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
    <!-- <div class="columns is-centered is-four-fifths">
      <div class="column is-four-fifths is-centered"> -->
          <img src="images/pipeline.png" alt="MY ALT TEXT" style="width: 100%;"/>
      </div>
    </div>
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths"> -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-five-fifths">
        <div class="content has-text-justified is-size-5">
          <p>
            (a) <strong>Two-step methods:</strong>  Existing range-view LiDAR generative models typically generate only depth and reflectance images, requiring an additional pre-trained 
            segmentation model to predict semantic labels. (b)<strong>SPIRAL:</strong>  In contrast, Spiral jointly generates depth, reflectance, and semantic maps. A closed-loop inference 
            mechanism (highlighted in the dash arrow) further improves cross-modal consistency.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero" id="Pipeline">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3">Pipeline</h2>
        <div class="content has-text-justified">    
        </div>
      </div>
    </div>
  </div>
  <br />
  <div class="container is-max-desktop">
    <!-- <div class="columns is-centered is-four-fifths">
      <div class="column is-four-fifths is-centered"> -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-five-fifths">
          <img src="images/method_new.png" alt="MY ALT TEXT" style="width: 100%;"/>
      </div>
    </div>
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths"> -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-five-fifths">
        <div class="content has-text-justified is-size-5">
          <p>
            (a) <strong>Unconditional Step:</strong>  Spiral takes noisy LiDAR scenes x<sub>t</sub> as input and predicts both the semantic map <span>ŷ<sub>t</sub></span> and the noise \(\hat{\epsilon}_t\), where the switch A is off and B is on. 
            (b) <strong>Conditional Step:</strong>  Spiral predicts \(\hat{\epsilon}_t\) conditioned on the given semantic map y smoothed by the progressive filter, where the switch A is on and B is off. 
            (c) <strong>Progressive Filter:</strong>  During inference, Spiral begins in an <strong>open-loop mode</strong> with unconditional denoising. Once the predicted semantic map reaches high confidence, it switches
            to a <strong>closed-loop</strong> mode that alternates between unconditional and conditional steps, enhancing cross-modal consistency.
      </div>
    </div>
  </div>
</section>


<section class="hero teaser" id="Video-Introduction">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <h2 class="title is-3" style="margin-top: 50px;">Visualization of the Generated LiDAR Scenes</h2>
      <div class="videos-grid">
        <!-- Video 1 -->
        <div class="video-item-2">
          <video poster="scene1" autoplay controls muted loop height="100%">
            <source src="videos/samples_0000000201.mp4" type="video/mp4">
          </video>
          <div class="content has-text-centered">
            <p>
              scene 1
            </p>
          </div>
        </div>
        <!-- Video 2 -->
        <div class="video-item-2">
          <video poster="" autoplay controls muted loop height="100%">
            <source src="videos/samples_0000000580.mp4" type="video/mp4">
          </video>
          <div class="content has-text-centered">
            <p>
              scene 2
            </p>
          </div>
        </div>
        <!-- Video 3 -->
        <div class="video-item-2">
          <video poster="" autoplay controls muted loop height="100%">
            <source src="videos/samples_0000000884.mp4" type="video/mp4">
          </video>
          <div class="content has-text-centered">
            <p>
              scene 3
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<style>
  .videos-grid {
    display: flex;
    flex-wrap: wrap;
    justify-content: space-between; /* Spread videos across the row */
    gap: 15px; /* Adjust spacing between videos */
    max-width: 1200px; /* Set a wider maximum width */
    margin: 0 auto; /* Center the grid */
  }
  .video-item {
    flex: 1 1 calc(50% - 20px); /* 3 columns */
    max-width: calc(50% - 20px);
    box-sizing: border-box;
  }
  .video-item-2 {
    flex: 1 1 calc(33% - 20px); /* 3 columns */
    max-width: calc(33% - 20px);
    box-sizing: border-box;
  }
  video {
    width: 100%;
    height: auto;
  }
  img {
    width: 100%;
  }
  strong {
    letter-spacing: normal;
    word-spacing: -0px;
    white-space: normal;
  }
  .green-text {
    color: rgb(84, 179, 69);
  }
  .gray-text {
    color: rgb(153, 153, 153);
  }
  .red-text {
    color: rgb(240, 67, 50);
  }
  .yellow-text {
    color: rgb(243, 210, 102);
  }
</style>



<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{zhu2025spiral,
      title={Spiral: Semantic-Aware Progressive LiDAR Scene Generation and Understanding},
      author={Zhu, Dekai and Hu, Yixuan and Liu, Youquan and Lu, Dongyue and Kong, Lingdong and Ilic, Slobodan},
      booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
      year={2025}
}</code></pre>
  </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> under the license <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>. We thank the authors for the open-source code.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
